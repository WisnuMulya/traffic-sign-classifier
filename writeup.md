# **Traffic Sign Recognition** 

---

**Build a Traffic Sign Recognition Project**

The goals / steps of this project are the following:
* Load the data set (see below for links to the project data set)
* Explore, summarize and visualize the data set
* Design, train and test a model architecture
* Use the model to make predictions on new images
* Analyze the softmax probabilities of the new images
* Summarize the results with a written report


## Rubric Points
### Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/481/view) individually and describe how I addressed each point in my implementation.  

---
### Writeup / README

#### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one. You can submit your writeup as markdown or pdf. You can use this template as a guide for writing the report. The submission includes the project code.

You're reading it! and here is a link to my [project code](https://github.com/WisnuMulya/traffic-sign-classifier)

### Data Set Summary & Exploration

#### 1. Provide a basic summary of the data set. In the code, the analysis should be done using python, numpy and/or pandas methods rather than hardcoding results manually.

The basic summary of the dataset could be read in the [jupyter notebook](./Traffic_Sign_Classifier.ipynb)
under the `Provide a Basic Summary of the Data Set Using Python, Numpy and/or Pandas` heading.

#### 2. Include an exploratory visualization of the dataset.

The exploratory visualization of the dataset could be viewed in the [jupyter notebook](./Traffic_Sign_Classifier.ipynb)
under the heading of `Plotting the traffic sign images` and `Plotting the data distribution`.

### Design and Test a Model Architecture

#### 1. Describe how you preprocessed the image data. What techniques were chosen and why did you choose these techniques? Consider including images showing the output of each preprocessing technique. Pre-processing refers to techniques such as converting to grayscale, normalization, etc. (OPTIONAL: As described in the "Stand Out Suggestions" part of the rubric, if you generated additional data for training, describe why you decided to generate additional data, how you generated the data, and provide example images of the additional data. Then describe the characteristics of the augmented training set like number of images in the set, number of images for each class, etc.)

The rationale behind the preprocessing techniques I adopt is inspired by the following:
- [Pierre Sermanet and Yann LeCun](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf)
    - A model trained using grayscale preprocessed images results in higher accuracy.
- [Scikit RGB to Grayscale Module](https://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_rgb_to_gray.html)
    - The way to implement RGB to Grayscale manually with numpy is by
    using the following formula: `Y = 0.2125 R + 0.7154 G + 0.0721 B`.
- [Online discussion on why we need to normalize images](https://stats.stackexchange.com/questions/185853/why-do-we-need-to-normalize-the-images-before-we-put-them-into-cnn)
    - Normalization is conducted, so that distribution of the features to be similar.
    
The visualization of the preprocessing techniques could be viewed in the notebook
under the heading `Visualize preprocessing steps`.

Further, since the distribution of the dataset images by labels shows that
some labels have high number of images (as high as 2010), while the other
have low number (as low as 180), this distribution might produce a model that
would underfit the classification of label with small dataset size.

To anticipate the underfitting, image augmentation would be an appropriate
approach to take. Also, image augmentation would make the model to be more
robust, since the augmentation images are generated by introducing more
variability.

The pipeline for generating image augmentation is the following:
- Apply a random affine transformation to the image using `scikit.transform` package
  that would include the following parameters:
    - Shear: stretch rotationally the image by `-0.05rad to 0.05rad`.
    - Translation: `-2px to 2px` for both x and y axis.
    - Scale: `0.9 to 1.1` for both x and y axis.
    - Rotate: rotate the image by `-pi/12 rad to pi/12 rad` (15deg).
- Apply random brightness using `scikit.exposure` module.
    - With gamma argument from `0.5 to 1.5`.
- ~~Apply random Gaussian noise with variance argument of `0.005`~~
    ~~found best by trial and error.~~
    **UPDATE: This is no longer applied, since it results in lower model**
    **accuracy.**
    
The visualization of the augmentation techniques could be viewed in the
notebook under the heading of `Visualize augmented images`.

Generating the augmentation dataset itself is abstracted by the
`GenerateAugmentImages` class, which acts as a generator that could be
fed the image data to be extrapolated from and output an augment image
dataset.

The orientation of determining the size of the augmentation dataset is
to have a roughly uniform distribution for each label in the final
training dataset, where the original test dataset and the augmentation
dataset are merged.

Finally, the augmentation dataset is saved under the `../data/augment.p`
to save time when needed to be load in the future.

#### 2. Describe what your final model architecture looks like including model type, layers, layer sizes, connectivity, etc.) Consider including a diagram and/or table describing the final model.

My final model consisted of the following layers:

| Layer         		|     Description	        					| 
|:---------------------:|:---------------------------------------------:| 
| Input         		| 32x32x1 one channel image						| 
| Convolution 5x5     	| 1x1 stride, valid padding, outputs 28x28x6 	|
| RELU					|												|
| Max pooling	      	| 2x2 stride,  outputs 14x14x6  				|
| Convolution 5x5	    | 1x1 stride, valid padding, outputs 10x10x16 	|
| RELU					|												|
| Max pooling	      	| 2x2 stride,  outputs 5x5x16  					|
| Flatten				| outputs 400									|
| Fully connected		| outputs 120  									|
| RELU					|												|
| Dropout				| 0.5 training keep probability					|
| Fully connected		| outputs 84  									|
| RELU					|												|
| Dropout				| 0.5 training keep probability					|
| Fully connected		| outputs 43  									|
 

#### 3. Describe how you trained your model. The discussion can include the type of optimizer, the batch size, number of epochs and any hyperparameters such as learning rate.

To train the model, I used an averaged softmax cross entropy for the cost 
function and AdamOptimizer for the optimizer, following the default setting
from the classroom.

The following parameters are used as results of trial and error process,
mostly on the learning rate and epochs:
- Learning rate = `0.0005`
- Epochs = `150`
- Batch Size = `128`

#### 4. Describe the approach taken for finding a solution and getting the validation set accuracy to be at least 0.93. Include in the discussion the results on the training, validation and test sets and where in the code these were calculated. Your approach may have been an iterative process, in which case, outline the steps you took to get to the final solution and why you chose those steps. Perhaps your solution involved an already well known implementation or architecture. In this case, discuss why you think the architecture is suitable for the current problem.

The final results of my model could be viewed in the jupyter notebook, under
the heading `Train, Validate and Test the Model`. Here are the results:
* Training set accuracy: 98.6%
* Validation set accuracy: 95.9%
* Test set accuracy: 93.7%

The approach I used in this project was an iterative one:
* First, I used the default model architecture from the classroom with learning rate,
    epochs, and batch size as above it resulted in around lower 80% validation set
    accuracy.
* Second, I preprocess the images by using grayscale and normalization techniques.
    It resulted in around 92% validation set accuracy, not yet 93%.
* Third, I decided to include dropout technique to improve my model, as it would
    make it more robust, would anticipate overfitting, and would then lead to higher 
    accuracy. It did and I passed the 93% validation set accuracy, but still
    oscilating to no more than 95%.
* Fourth, I decided to augment the image data as a way to improve my model accuracy
    with the approach I outlined in point # 1 above. I generated around 70k images. 
    With this, I first achieved the validation set accuracy of around 93%, but not 
    higher. I tried it twice by lowering the learning rate and increasing the 
    epochs, but it gets even worse.
* Fifth, I made a guess that the gaussian noise worsen the performance of the model.
    Thus, I had removed it from the augmentation techniques and the model
    intantly got better: ~96% validation accuracy.
* Sixth, I increased my aumentation image size and generated around 170k images, so
    including the original one, the train dataset was around 200k in size. However,
    this approach worsen my model: 93% and not higher. This might be due to
    overfitting, thus I decided to reduce the number of augmented images.
* Seventh, I ended up with training dataset with ~110k in size (~74k augmented). This
    final iteration resulted in the accuracy mentioned above.
    
Future development:
* After a long, iterative, tinkering with the parameters, techniques, and methods, a
    future development on the model might address the problem of **overfitting** by
    including the measure of regularization, such as using the **L2 Regularization** as
    the cost function.
* Further, the model might benefit with a more complex techniques of augmentation, since
    it appeared that, even after generating the augmentation images with the techniques
    I employed, the model still facing the overfitting problem, which suggests that the
    augmentation images might not introduce enough variability for the model to learn
    better.

### Test a Model on New Images

#### 1. Choose five German traffic signs found on the web and provide them in the report. For each image, discuss what quality or qualities might be difficult to classify.

The visualization of the new images could be viewed in the [jupyter notebook](./Traffic_Sign_Classifier.ipynb)
under the `Step 3: Test a Model on New Images` > `Load and Output the Images` heading.

Discussion on the images is also provided in the notebook under the same heading.

#### 2. Discuss the model's predictions on these new traffic signs and compare the results to predicting on the test set. At a minimum, discuss what the predictions were, the accuracy on these new predictions, and compare the accuracy to the accuracy on the test set (OPTIONAL: Discuss the results in more detail as described in the "Stand Out Suggestions" part of the rubric).

The prediction of the new images could be viewed in the [jupyter notebook](./Traffic_Sign_Classifier.ipynb)
under the `Step 3: Test a Model on New Images` > `Predict the Sign Type for Each Image` heading.

Discussion on the pediction is also provided in the notebook under the same heading.

Here are the results of the prediction:

#### 3. Describe how certain the model is when predicting on each of the five new images by looking at the softmax probabilities for each prediction. Provide the top 5 softmax probabilities for each image along with the sign type of each probability. (OPTIONAL: as described in the "Stand Out Suggestions" part of the rubric, visualizations can also be provided such as bar charts)

The accuracy of the prediction on the new images could be viewed in the
[jupyter notebook](./Traffic_Sign_Classifier.ipynb) under the
`Step 3: Test a Model on New Images` > `Output Top 5 Softmax Probabilities For Each Image Found on the Web`
heading.

Discussion on it is also provided in the notebook under the same heading.

The code for making predictions on my final model is located in the 11th cell of the Ipython notebook.
